{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training neural networks efficiently with high-level TensorFlow APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building multilayer neural networks using TensorFlow's Layers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets use a high-level tf.layers API to train a multilayer neural network model for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzips mnist\n",
    "\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "if (sys.version_info > (3, 0)):\n",
    "    writemode = 'wb'\n",
    "else:\n",
    "    writemode = 'w'\n",
    "\n",
    "zipped_mnist = [f for f in os.listdir('./') if f.endswith('ubyte.gz')]\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z, mode='rb') as decompressed, open(z[:-3], writemode) as outfile:\n",
    "        outfile.write(decompressed.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the data\n",
    "X_train, y_train = load_mnist('.', kind='train')\n",
    "print('Rows: %d,  Columns: %d' %(X_train.shape[0], \n",
    "                                 X_train.shape[1]))\n",
    "\n",
    "X_test, y_test = load_mnist('.', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' %(X_test.shape[0], \n",
    "                                     X_test.shape[1]))\n",
    "## mean centering and normalization:\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - create three fully connected neuron layers using high level tf.layers API\n",
    "# in the hidden layer, we will use hyperbolic tangent activation functions (tanh), and softmax in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_features = X_train_centered.shape[1]\n",
    "n_classes = 10\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    tf_x = tf.placeholder(dtype=tf.float32,\n",
    "                       shape=(None, n_features),\n",
    "                       name='tf_x')\n",
    "\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, \n",
    "                        shape=None, name='tf_y')\n",
    "    y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n",
    "\n",
    "    h1 = tf.layers.dense(inputs=tf_x, units=50,\n",
    "                         activation=tf.tanh,\n",
    "                         name='layer1')\n",
    "\n",
    "    h2 = tf.layers.dense(inputs=h1, units=50,\n",
    "                         activation=tf.tanh,\n",
    "                         name='layer2')\n",
    "\n",
    "    logits = tf.layers.dense(inputs=h2, \n",
    "                             units=10,\n",
    "                             activation=None,\n",
    "                             name='layer3')\n",
    "\n",
    "    predictions = {\n",
    "        'classes' : tf.argmax(logits, axis=1, \n",
    "                              name='predicted_classes'),\n",
    "        'probabilities' : tf.nn.softmax(logits, \n",
    "                              name='softmax_tensor')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define cost function and optimizer:\n",
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=y_onehot, logits=logits)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=0.001)\n",
    "\n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a session to launch the graph\n",
    "sess =  tf.Session(graph=g)\n",
    "## run the variable initialization operator\n",
    "sess.run(init_op)\n",
    "\n",
    "## 50 epochs of training:\n",
    "training_costs = []\n",
    "for epoch in range(50):\n",
    "    training_loss = []\n",
    "    batch_generator = create_batch_generator(\n",
    "            X_train_centered, y_train, \n",
    "            batch_size=64)\n",
    "    for batch_X, batch_y in batch_generator:\n",
    "        ## prepare a dict to feed data to our network:\n",
    "        feed = {tf_x:batch_X, tf_y:batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost],\n",
    "                                 feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(' -- Epoch %2d  '\n",
    "          'Avg. Training Loss: %.4f' % (\n",
    "              epoch+1, np.mean(training_costs)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do prediction on the test set:\n",
    "feed = {tf_x : X_test_centered}\n",
    "y_pred = sess.run(predictions['classes'], \n",
    "                  feed_dict=feed)\n",
    " \n",
    "print('Test Accuracy: %.2f%%' % (\n",
    "      100*np.sum(y_pred == y_test)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Multilayer Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - Keras is another high level neural network API and is part of TensorFlow.Contrib module from 1.1.0 version of\n",
    "#TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('./', kind='train')\n",
    "print('Rows: %d,  Columns: %d' %(X_train.shape[0], \n",
    "                                 X_train.shape[1]))\n",
    "X_test, y_test = load_mnist('./', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' %(X_test.shape[0], \n",
    "                                 X_test.shape[1]))\n",
    "\n",
    "## mean centering and normalization:\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    " \n",
    "del X_train, X_test\n",
    " \n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - convert class labels 0-9 into one hot vector using a Keras tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train)\n",
    " \n",
    "print('First 3 labels: ', y_train[:3])\n",
    "print('\\nFirst 3 labels (one-hot):\\n', y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - now define the 3 layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=50,    \n",
    "        input_dim=X_train_centered.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=50,    \n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        units=y_train_onehot.shape[1],    \n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='softmax'))\n",
    "\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "        lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - we are using mini-batch stochastic gradient descent here.\n",
    "#The validation_split parameter is especially handy since it will reserve 10 percent\n",
    "#of the training data (here, 6,000 samples) for validation after each epoch so that we\n",
    "#can monitor whether the model is overfitting during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_centered, y_train_onehot,\n",
    "                    batch_size=64, epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CellStrat - let's predict some values using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - print accuracy of predictions for training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, \n",
    "                                     verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0) \n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "\n",
    "print('First 3 predictions: ', y_train_pred[:3])\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, \n",
    "                                    verbose=0)\n",
    "\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0) \n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
