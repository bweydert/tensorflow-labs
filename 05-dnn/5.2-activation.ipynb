{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing activation functions for multilayer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic function recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|x) = 0.888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X = np.array([1, 1.4, 2.5]) ## first value must be 1 for x0 bias parameter\n",
    "w = np.array([0.4, 0.3, 0.5])\n",
    "\n",
    "def net_input(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "print('P(y=1|x) = %.3f' % logistic_activation(X, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above result shows that probability of y=1 is 88.8%.\n",
    "#Normally if sigmoid probability > 0.5, we mark the final output as 1 or TRUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code below shows an output layer consisting of multiple logistic activation\n",
    "#units does not produce meaningful, interpretable probability values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Input: \n",
      " [1.78 0.76 1.65]\n",
      "Output Units:\n",
      " [0.85569687 0.68135373 0.83889105]\n"
     ]
    }
   ],
   "source": [
    "# W : array with shape = (n_output_units, n_hidden_units+1)\n",
    "#     note that the first column are the bias units\n",
    "\n",
    "W = np.array([[1.1, 1.2, 0.8, 0.4],\n",
    "              [0.2, 0.4, 1.0, 0.2],\n",
    "              [0.6, 1.5, 1.2, 0.7]])\n",
    "\n",
    "# A : data array with shape = (n_hidden_units + 1, n_samples)\n",
    "#     note that the first column of this array must be 1\n",
    "\n",
    "A = np.array([[1, 0.1, 0.4, 0.6]])\n",
    "\n",
    "Z = np.dot(W, A[0])\n",
    "y_probas = logistic(Z)\n",
    "\n",
    "print('Net Input: \\n', Z)\n",
    "\n",
    "print('Output Units:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see in the output, the resulting values cannot be interpreted as\n",
    "#probabilities for a three-class problem. The reason for this is that they do not sum up\n",
    "#to 1. However, this is in fact not a big concern if we only use our model to predict\n",
    "#the class labels, not the class membership probabilities. One way to predict the class\n",
    "#label from the output units obtained earlier is to use the maximum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: 0\n"
     ]
    }
   ],
   "source": [
    "y_class = np.argmax(Z, axis=0)\n",
    "print('Predicted class label: %d' % y_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating class probabilities in multi-class classification via the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax function can be used to calculate multi-class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [0.44668973 0.16107406 0.39223621]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - the total of probabilities adds upto 1 now.\n",
    "#Intuitively, it may help to think of the softmax function as a normalized output that is useful to obtain meaningful\n",
    "#classmembership predictions in multiclass settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadening the output spectrum by using a hyperbolic tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CellStrat - the hyperbolic tangent or tanh creates a broader output spectrum from -1 to +1. This can improve the convergence\n",
    "# during backpropagation.\n",
    "#Let's plot both signoid and tanh functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tanh(z):\n",
    "    e_p = np.exp(z)\n",
    "    e_m = np.exp(-z)\n",
    "    return (e_p - e_m) / (e_p + e_m)\n",
    "\n",
    "z = np.arange(-5, 5, 0.005)\n",
    "log_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlabel('net input $z$')\n",
    "plt.ylabel('activation $\\phi(z)$')\n",
    "plt.axhline(1, color='black', linestyle=':')\n",
    "plt.axhline(0.5, color='black', linestyle=':')\n",
    "plt.axhline(0, color='black', linestyle=':')\n",
    "plt.axhline(-0.5, color='black', linestyle=':')\n",
    "plt.axhline(-1, color='black', linestyle=':')\n",
    "\n",
    "plt.plot(z, tanh_act,\n",
    "         linewidth=3, linestyle='--',\n",
    "         label='tanh')\n",
    "\n",
    "plt.plot(z, log_act,\n",
    "         linewidth=3,\n",
    "         label='logistic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
